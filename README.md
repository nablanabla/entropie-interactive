# Les concepts fondamentaux de l'apprentissage machine : entropie et entropie croisÃ©e 

[![Deployed](https://img.shields.io/badge/statut-en%20ligne-brightgreen)](https://VOTRE-USERNAME.github.io/entropie-interactive/)
[![License](https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-blue)](https://creativecommons.org/licenses/by-nc-sa/4.0/)

> **ActivitÃ© pÃ©dagogique interactive** pour dÃ©couvrir le concept d'entropie de Shannon et son utilisation en Apprentissage Machine.

---
# ğŸ“– ActivitÃ© 1 - L'entropie

## ğŸ¯ Objectif

Cette activitÃ© permet de s'initier Ã  la thÃ©orie de l'information de Claude Shannon et de discuter son application dans le domaine de l'intelligence artificielle.


---


## ğŸ“š Contexte historique

<p align="center">
  <img src="assets/images/Image1.jpg" alt="Shannon Schema" width="400">
  <br>
  <em>Figure 1 : SchÃ©ma de communication de Shannon (1948)</em>
</p>


Shannon est un pionnier involontaire de l'Intelligence Artificielle. Dans un cÃ©lÃ¨bre article de 1948, il aborde la question suivante : comment peser l'information qui, Ã©mise depuis un Ã©metteur sous forme de message, passe dans un canal de diffusion â€” Ã©ventuellement bruitÃ© â€” pour ensuite Ãªtre reproduit pour un rÃ©cepteur ? (Shannon, 1948, fig.2).  



<p align="center">
  <img src="assets/images/Image2.jpg" alt="Concept Entropie" width="700">
  <br>
  <em>Figure 2 : Le problÃ¨me initial de Shannon (1948).</em>
</p>

"Messages" et "transmetteurs" sont Ã  prendre dans un sens trÃ¨s large, ils peuvent dÃ©signer :

| Messages | Transmetteurs |
|----------|---------------|
| Des textes | Un microphone tÃ©lÃ©phonique (qui convertit la voix en signaux Ã©lectriques) |
| Des signaux vocaux | Un Ã©quipement tÃ©lÃ©graphique (codant les lettres en impulsions) |
| Des images | Un Ã©metteur radio (modulant l'information sur une onde porteuse) |
| Des donnÃ©es binaires | Un circuit Ã©lectrique avec des portes logiques "ouvert/fermÃ©" |


La dÃ©marche de Shannon est prÃ©curseur de l'informatique thÃ©orique, dans la forme qu'on lui connaÃ®t maintenant (le mot informatique sera introduit bien plus tard dans les annÃ©es 70). Bien que centrÃ© sur des problÃ¨mes de communication, il donne dans cet article un sens au mot information qui n'est pas reliÃ© au *sens* de cette information. Il faut comprendre l'importance de cette remarque : Shannon distingue Ã  priori information et sÃ©mantique, ce qui a Ã©tÃ© un point de bascule majeur pour le dÃ©veloppement de l'informatique. L'article introduit deux concepts majeurs. Tout d'abord, l'unitÃ© de l'information est le *bit* (un nombre dÃ©cimal). Ensuite, la mesure de l'information sera une fonction mathÃ©matique que Shannon dÃ©cide d'appeler *entropie* en relation avec le concept du mÃªme nom en thermodynamique.

L'activitÃ© consiste Ã  dÃ©couvrir le point suivant :

**L'entropie correspond au nombre minimum thÃ©orique de bits qu'il faut pour coder un message.**

---

## ğŸ“š Valeur de l'information et incertitude

L'observation clÃ© de Shannon est qu'un message est d'autant plus informatif que sa probabilitÃ© d'apparaÃ®tre est faible. De plus, Shannon fait un astucieux parallÃ¨le entre *information* d'un cÃ´tÃ© et *incertitude* de l'autre: l'incertitude est d'autant plus forte qu'il faut beaucoup d'information pour la combler. 

Ces deux remarques sont au coeur de l'apprentissage machine.

### Exemple 1
Imaginons que l'on soit un 27 fÃ©vrier au QuÃ©bec. Le message ***"Il fait froid aujourd'hui"*** est peu informatif, car sa probabilitÃ© d'apparaÃ®tre un 27 fÃ©vrier est trÃ¨s forte. En revanche le message ***"Il fait 15Â°C aujourd'hui"*** est Ã  trÃ¨s haute valeur informative car sa probabilitÃ© est trÃ¨s trÃ¨s faible. De fait, cet Ã©vÃ¨nement exceptionnel est arrivÃ© une fois, en 2024 comme le relate [l'article suivant](https://www.journaldemontreal.com/2024/02/27/chaud-pour-un-mois-de-fevrier-des-records-de-temperature-battus-mardi-au-quebec).

### Exemple 2
PlaÃ§ons nous maintenant dans la peau d'un mÃ©decin qui reÃ§oit un patient. Si le patient dÃ©clare ***"j'ai mal Ã  la gorge"*** (trÃ¨s frÃ©quent et donc probable), le mÃ©decin reste dans une forte ***incertitude*** car le spectre de diagnostic est trÃ¨s trÃ¨s large Ã  ce stade, il aura besoin de beaucoup plus d'information pertinente pour Ã©tablir un diagnostic diffÃ©rentiel.  En revanche, si le patient dÃ©clare ***"je viens vous voir car je me suis fait mordre par un Mamba noir"*** (trÃ¨s peu probable), cela apporte beaucoup d'information au mÃ©decin. Ici il sait mÃªme immÃ©diatement ce qu'il faut faire (injecter un sÃ©rum antivenimeux) ! 




---

## ğŸŒ Pour clarifier ces diffÃ©rents concepts, allons Ã  la rencontre de l'entropie

<p align="center">
  <a href="https://nablanabla.github.io/entropie-interactive/activite-entropie/" target="_blank" rel="noopener noreferrer">
    <strong>ğŸš€ Lancer l'activitÃ© interactive "Ã€ la dÃ©couverte de l'entropie"</strong>
  </a>
</p>



---



# ğŸ“– ActivitÃ© 2 - L'entropie croisÃ©e

## ğŸ“š Contexte historique

En 1951, Solomon Kullback et Richard Leibler Ã©tendent le concept d'entropie de Shannon (qui ne mesure l'incertitude que d'une seule distribution) pour mesurer la distance entre deux distributions statistiques.

<p align="center">
  <img src="assets/images/image3.png" alt="Kullbach-Leiber article" width="400">
  <br>
  <em>Figure 3 : L'article fondateur de Kullback et Leiber (1951)</em>
</p>


### ProblÃ©matique: comment mesurer l'Ã©cart entre deux distributions de probabilitÃ© ?

Cette question est fondamentale en apprentissage machine : elle permet d'Ã©valuer Ã  quel point une prÃ©diction s'Ã©carte de la rÃ©alitÃ©.

### ğŸ©º Exemple : Ã‰valuer deux Ã©tudiants rÃ©sidents en mÃ©decine 


Deux Ã©tudiants en mÃ©decine doivent Ã©tablir un diagnostic diffÃ©rentiel pour un patient prÃ©sentant des symptÃ´mes complexes. Un expert a analysÃ© le cas et donne sa distribution de probabilitÃ© pour 4 diagnostics possibles. On soumet ces diagnostics Ã  deux Ã©tudiants en mÃ©decine qui doivent Ã©tablir leur propre distribution de probabilitÃ© (sans connaÃ®tre celle de l'expert Ã©videmment). 

**Les distributions de probabilitÃ© obtenus :**

| | **Dâ‚** | **Dâ‚‚** | **Dâ‚ƒ** | **Dâ‚„** | 
|---|---|---|---|---|
| **Expert (P)** | **80%** | 10% | 5% | 5% | 
| **Ã‰tudiant 1 (R1)** | 50% | 20% | 15% | 15% | 
| **Ã‰tudiant 2 (R2)** | 25% | 25% | 25% | 25% | 

**Questions : 1/ quel Ã©tudiant est le plus proche de l'expert ? et 2/ Ã€ quel point le plus loin est loin ?**
Pour y rÃ©pondre passer Ã  l'activitÃ© 2:

<p align="center">
  <a href="https://nablanabla.github.io/entropie-interactive/activite-entropie-croisee/" target="_blank" rel="noopener noreferrer">
    <strong>ğŸš€ Lancer l'ActivitÃ© 2 : "L'entropie croisÃ©e"</strong>
  </a>
</p>

---

## ğŸ“š Pour conclure: DiffÃ©rence entre information au sens de Shannon et pertinence diagnostique

### Le piÃ¨ge conceptuel

La thÃ©orie de Shannon mesure l'**information** comme la "surprise" : un Ã©vÃ©nement rare (p faible) apporte beaucoup d'information car il est inattendu.

**Mais attention :** En mÃ©decine, un symptÃ´me peut Ãªtre trÃ¨s **informatif au sens de Shannon** (rare, surprenant) ET pourtant **peu utile diagnostiquement** (non spÃ©cifique).

### Les quatre cas possibles

| | **SymptÃ´me frÃ©quent** (p Ã©levÃ©) | **SymptÃ´me rare** (p faible) |
|---|---|---|
| **SymptÃ´me spÃ©cifique**<br>(peu de diagnostics compatibles) | ğŸ“‰ **Faible** information Shannon<br>âœ… **Haute** pertinence diagnostique<br><br>*Exemple : FiÃ¨vre dans la grippe*<br>SymptÃ´me banal mais qui oriente bien | ğŸ“ˆ **Haute** information Shannon<br>âœ… **Haute** pertinence diagnostique<br><br>*Exemple : Ã‰ruption lupique pathognomonique*<br>Rare ET spÃ©cifique = diagnostic quasi certain |
| **SymptÃ´me non spÃ©cifique**<br>(beaucoup de diagnostics compatibles) | ğŸ“‰ **Faible** information Shannon<br>âŒ **Faible** pertinence diagnostique<br><br>*Exemple : Fatigue vague*<br>Banal et compatible avec 100 maladies | ğŸ“ˆ **Haute** information Shannon<br>âŒ **Faible** pertinence diagnostique<br><br>*Exemple : SymptÃ´me jamais dÃ©crit*<br>Surprenant mais on ne sait pas l'interprÃ©ter |



Les deux concepts sont **indÃ©pendants** ! Un symptÃ´me peut Ãªtre trÃ¨s surprenant (haute information Shannon) sans pour autant rÃ©duire l'incertitude diagnostique.

---
## ğŸ“– Conclusion de ce parcours

La thÃ©orie de l'information permet de :
- âœ… Comprendre comment **mesurer l'information** et la relier au concept d'incertitude
- âœ… Voir comment les **LLMs optimisent leurs prÃ©dictions** avec l'entropie croisÃ©e

âŒ MAISlLa thÃ©orie de l'information ne permet **pas** d'Ã©valuer la pertinence de l'information pour le clinicien !!

---


## ğŸ“ Utilisation pÃ©dagogique

### Pour les enseignants

Cette activitÃ© vise Ã  faire un dÃ©couvrir le concept d'information au sens de l'informatique

**DurÃ©e estimÃ©e :** 20-30 minutes

---

## ğŸ“ RÃ©fÃ©rences

Shannon, C. E. (1948). A mathematical theory of communication. *The Bell System Technical Journal*, **27**(3), 379â€“423.


Kullback, S., & Leibler, R. A. (1951). On information and sufficiency. *The Annals of Mathematical Statistics*, **22**(1), 79â€“86.

---

## ğŸ‘¨â€ğŸ« Auteur

**Alban Da Silva**  
ChargÃ© d'Enseignement en MÃ©decine - FacultÃ© de MÃ©decine  
UniversitÃ© Laval, QuÃ©bec, Canada

**Contexte :** Cours "Culture NumÃ©rique en Sciences de la SantÃ©"

---

## ğŸ“„ Licence

Ce contenu pÃ©dagogique est sous licence [Creative Commons BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/).

Vous Ãªtes libre de :
- âœ… **Partager** 
- âœ… **Adapter** 

Sous les conditions suivantes :
- ğŸ“ **Attribution** â€” crÃ©diter l'auteur
- ğŸš« **Pas d'utilisation commerciale**
- ğŸ”„ **Partage dans les mÃªmes conditions**

---

## ğŸ¤ Contributions

Les suggestions d'amÃ©lioration sont les bienvenues ! 

**Pour signaler un bug ou proposer une amÃ©lioration :**
- Ouvrir une [Issue](https://github.com/VOTRE-USERNAME/entropie-interactive/issues)
- Ou me contacter directement

---


## ğŸ“… Historique des versions

- **v1.0** (FÃ©vrier 2026) - Version initiale avec 2 activitÃ©s interactives


---

## ğŸ’¡ Technologies utilisÃ©es

![HTML5](https://img.shields.io/badge/HTML5-E34F26?style=flat&logo=html5&logoColor=white)
![CSS3](https://img.shields.io/badge/CSS3-1572B6?style=flat&logo=css3&logoColor=white)
![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=flat&logo=javascript&logoColor=black)
![GitHub Pages](https://img.shields.io/badge/GitHub%20Pages-222222?style=flat&logo=githubpages&logoColor=white)
![KaTeX](https://img.shields.io/badge/KaTeX-228B22?style=flat&logo=latex&logoColor=white)
---

â­ **Si cette activitÃ© vous a Ã©tÃ© utile, n'hÃ©sitez pas Ã  mettre une Ã©toile sur le dÃ©pÃ´t !**
